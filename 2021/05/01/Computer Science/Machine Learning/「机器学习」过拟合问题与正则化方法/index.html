<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>「机器学习」过拟合问题与正则化方法 | J1z's Blog</title><meta name="keywords" content="Machine Learning"><meta name="author" content="Jay"><meta name="copyright" content="Jay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="机器学习中，如果参数过多，模型过于复杂，容易造成过拟合（overfitting）：即模型在训练样本数据上表现的很好，但在实际测试样本上表现的较差，不具备良好的泛化能力。 为了避免过拟合，最常用的一种方法是使用正则化，例如 L1 和 L2 正则化。但是，正则化项是如何得来的？其背后的数学原理是什么？L1 正则化和 L2 正则化之间有何区别？除此之外，还有哪些方法来解决过拟合问题？ 本文将对这些问题给">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习」过拟合问题与正则化方法">
<meta property="og:url" content="http://jay1zhang.github.io/2021/05/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="J1z&#39;s Blog">
<meta property="og:description" content="机器学习中，如果参数过多，模型过于复杂，容易造成过拟合（overfitting）：即模型在训练样本数据上表现的很好，但在实际测试样本上表现的较差，不具备良好的泛化能力。 为了避免过拟合，最常用的一种方法是使用正则化，例如 L1 和 L2 正则化。但是，正则化项是如何得来的？其背后的数学原理是什么？L1 正则化和 L2 正则化之间有何区别？除此之外，还有哪些方法来解决过拟合问题？ 本文将对这些问题给">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/014.jpg">
<meta property="article:published_time" content="2021-05-01T02:43:46.000Z">
<meta property="article:modified_time" content="2021-05-01T10:32:42.709Z">
<meta property="article:author" content="Jay">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/014.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://jay1zhang.github.io/2021/05/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"WKEK6XV2V5","apiKey":"e093fbb9f29fdf5c7ddd56ec43e9ae05","indexName":"Jay","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-01 18:32:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">57</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/014.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">J1z's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">「机器学习」过拟合问题与正则化方法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-05-01T02:43:46.000Z" title="Created 2021-05-01 10:43:46">2021-05-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-05-01T10:32:42.709Z" title="Updated 2021-05-01 18:32:42">2021-05-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/">Computer Science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>8min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>机器学习中，如果参数过多，模型过于复杂，容易造成过拟合（overfitting）：即模型在训练样本数据上表现的很好，但在实际测试样本上表现的较差，不具备良好的泛化能力。</p>
<p>为了避免过拟合，最常用的一种方法是使用正则化，例如 L1 和 L2 正则化。但是，正则化项是如何得来的？其背后的数学原理是什么？L1 正则化和 L2 正则化之间有何区别？除此之外，还有哪些方法来解决过拟合问题？</p>
<p>本文将对这些问题给出直观的解释。</p>
<h2 id="0x00-过拟合问题"><a href="#0x00-过拟合问题" class="headerlink" title="0x00 过拟合问题"></a>0x00 过拟合问题</h2><p>虽然很简单，但我们还是先来回顾一下过拟合问题，以线性回归中的房价预测为例：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501161346760.png" alt="image-20210501161346760" style="zoom:67%;" />



<ul>
<li><p>左边第一幅图是一个线性模型，并没有很好的拟合训练数据，我们把此类情况称为<strong>欠拟合(underfitting)**，或者叫作叫做</strong>高偏差(bias)**。</p>
</li>
<li><p>中间第二幅图，我们在中间加入一个二次项，也就是说对于这幅数据我们用二次函数去拟合，事实也证明这个拟合效果很好，我们称之为 just right。</p>
</li>
<li><p>右边第三幅图则是一个更加极端的情况，我们使用一个四次多项式来拟合。通过我们的五个训练样本，我们可以得到如右图的一条曲线。</p>
<ul>
<li>一方面，我们似乎对训练数据做了一个很好的拟合，因为这条曲线通过了所有的训练实例。但是，这实际上是一条很扭曲的曲线，它不停上下波动。因此，事实上我们并不认为它是一个预测房价的好模型。</li>
<li>所以，我们把这类情况叫做<strong>过拟合(overfitting)**，也叫</strong>高方差(variance)**。</li>
</ul>
</li>
</ul>
<p>过拟合问题通常发生在<strong>变量（特征）过多</strong>且<strong>没有足够的数据集（训练集）去约束这个模型</strong>的时候。这种情况下训练出的方程总是能很好的拟合训练数据，也就是说，我们的代价函数在训练集上可能非常接近于 0，甚至就为0。</p>
<p>但是，这样的曲线千方百计的去拟合训练数据，可能会导致它无法<strong>泛化</strong>到新的数据样本中，即无法预测新样本价格。</p>
<p>这就是过拟合问题，下面这个图可能更加有趣一些：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501155541011.png" alt="image-20210501155541011" style="zoom:80%;" />





<h2 id="0x01-加入正则项"><a href="#0x01-加入正则项" class="headerlink" title="0x01 加入正则项"></a>0x01 加入正则项</h2><p>为了防止过拟合，可以<strong>对模型添加正则化项来限制模型的复杂度</strong>，使得模型在复杂度和性能之间达到平衡。</p>
<p>考虑前面的房价预测的例子，我们的优化目标是最小化代价函数 $J(\theta)$：</p>
<p>$$J(\theta) =  \frac{1}{2m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})^2 $$</p>
<p>如果我们在代价函数中加入两个惩罚项来对高阶参数 $\theta_3$ 和 $\theta_4$ 进行惩罚：</p>
<p>$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})^2 + \lambda |\theta_3|^2 + \lambda |\theta_4|^2 $$</p>
<p>当 $\lambda$ 取得很大时，比如1000，那新的代价函数将会变得非常大。</p>
<p>所以当我们在最小化这个新的代价函数时， 我们将使 $\theta_3$ 和 $\theta_4$ 这两个参数的值都接近于 0，那么我们将得到一个近似的二次函数，如下图。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501162816848.png" alt="image-20210501162816848" style="zoom:67%;" />



<p>通过加入正则项的方法，我们最终恰当地拟合了训练数据，且很好地控制了模型的复杂度。</p>
<p>这就是正则化背后的核心思路，即</p>
<blockquote>
<p><strong>如果我们的参数值越小，通常对应于越光滑的函数，也就是更加简单的函数，因此就不易发生过拟合的问题。</strong></p>
</blockquote>
<p>但还有一个问题是，如果考虑更一般的情况，比如当我们有一百个特征时，我们并不知道该如何选择关联度更好的参数，如何缩小参数的数目等等。因此在正则化中，我们就需要对所有的参数值进行约束：</p>
<p>$$J(\theta) =  \frac{1}{2m} [\ \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n |\theta_j|^2 \ ]$$</p>
<p>其中，$\lambda \sum_{j=1}^n |\theta_j|^2 $ 就是<strong>正则项</strong>，$\lambda$ 被称为<strong>正则化参数</strong>。</p>
<h2 id="0x02-L1、L2-正则化"><a href="#0x02-L1、L2-正则化" class="headerlink" title="0x02 L1、L2 正则化"></a>0x02 L1、L2 正则化</h2><p>前面已经介绍了加入正则项的作用与原理，L1和L2正则化的不同仅在于使用的正则项不同，使用L1正则化的模型叫做Lasso回归（拉索回归），使用L2正则化的模型叫做Ridge回归（岭回归），下面我们一一介绍。</p>
<h3 id="1-L1、L2-正则化的原理"><a href="#1-L1、L2-正则化的原理" class="headerlink" title="1. L1、L2 正则化的原理"></a>1. L1、L2 正则化的原理</h3><p>L2 正则化公式非常简单，实际上我们前面就是以 L2 为例介绍的，即直接在原来的损失函数基础上加上权重参数的平方和：</p>
<p>$$J = J_0 + \lambda \sum_j |w_j|^2 $$</p>
<p>L1 正则化则是参数的绝对值之和：</p>
<p>$$J = J_0 + \lambda \sum_j |w_j| $$</p>
<h3 id="2-L1、L2-正则化的直观解释"><a href="#2-L1、L2-正则化的直观解释" class="headerlink" title="2. L1、L2 正则化的直观解释"></a>2. L1、L2 正则化的直观解释</h3><p>那怎么去理解呢？我们知道，正则化的目的是为了对模型进行约束，避免模型更加复杂。为了达到这一目的，最直观的方法就是<strong>限制参数 $w$ 的个数</strong>，但是这类条件属于 NP-hard 问题，求解非常困难。所以，一般的做法是寻找更宽松的限定条件：</p>
<p>$$\sum_j |w_j|^2 \leq C$$</p>
<p>即对参数 $w$ 做数值上的限定，使 <strong>正则项之和不超过常数 $C$ ** ，从而</strong>避免参数过大**。</p>
<p>这样，我们的目标就变成了在<strong>不等式条件约束下的最优化问题</strong>。</p>
<p>为了更形象化的说明，我们假设待优化目标在二维空间进行，即只有两个参数 $w_1$ 和 $w_2$ 。</p>
<p>那么 L2 正则化对解空间的约束为：</p>
<p>$$w_1^2 + w_1^2 \leq C$$</p>
<p>L1 正则化对解空间的约束为：</p>
<p>$$|w_1| + |w_2| \leq C$$</p>
<p>在二维空间中绘制两个式子的图像，可知L2约束的范围是一个圆形，L1约束的范围是一个菱形，且顶点在坐标轴上。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501175221821.png" alt="image-20210501175221821" style="zoom:75%;" />



<blockquote>
<p>上图中，左边是L2约束下解空间的图像，右边是L1约束下解空间的图像，蓝色的圆圈表示损失函数的等值线。<strong>同一个圆上的损失函数值相等的，圆的半径越大表示损失值越大</strong>，由外到内，损失函数值越来越小，中间最小。</p>
</blockquote>
<p>我们发现，如果没有L1和L2正则化约束的话，$w_1$ 和 $w_2$ 是可以任意取值的，即<strong>损失函数可以优化到中心的最小值的</strong>，此时中心对应的 $w_1$ 和 $ w_2$ 的取值就是模型最终求得的参数。</p>
<p>但是加入了L1和L2正则化约束就把<strong>解空间约束在了黄色的平面内</strong>。黄色图像的边缘与损失函数等值线的交点，便是<strong>同时满足约束条件且能够使损失函数最小化的参数的解</strong>。</p>
<h3 id="3-正则化参数-lambda"><a href="#3-正则化参数-lambda" class="headerlink" title="3. 正则化参数 $\lambda$"></a>3. 正则化参数 $\lambda$</h3><p>正则项中的参数 $\lambda$ 实际上起到了权衡的作用。</p>
<p>以 L2 为例，若 $\lambda$ 很小，对应上文中的 C 值就会很大。这时候，圆形区域就很大，因而能够让 $w$ 更加接近最优解的位置（损失函数等值线的中心点）。但如果 $\lambda$ 近似为 0，圆形区域就会覆盖最优解的位置，这时候，正则化失效，便容易造成过拟合。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501181617569.png" alt="image-20210501181617569" style="zoom:80%;" />

<p>相反，如果 $\lambda$ 很大，那对应的 C 值就很小。这时候，圆形区域很小，$w$ 距离最优解的位置较远，被限制在一个很小的区域内变化，$w$ 普遍较小且接近 0。虽然起到了正则化的效果，但却容易造成欠拟合。</p>
<h3 id="4-L1、L2-正则化的特点"><a href="#4-L1、L2-正则化的特点" class="headerlink" title="4. L1、L2 正则化的特点"></a>4. L1、L2 正则化的特点</h3><p>L2正则化可以直观理解为它会对于大数值的权重向量进行严厉惩罚，因而倾向于更加数值更小更加分散的权重向量。这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。 这样做可以提高模型的泛化能力，降低过拟合的风险。</p>
<p>在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</p>
<p>但L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后得到的是输入数据中最重要的稀疏子集，因而可以<strong>用于特征选择</strong>。</p>
<blockquote>
<p>这是由于，L1正则化约束的解空间是一个菱形，因而等值线与菱形端点相交的概率比与线的中间相交的概率要大很多，而端点在坐标轴上，即会有一部分参数取值为0。因而<strong>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型</strong>。</p>
<p>而L2正则化约束的解空间是圆形，所以等值线与圆的任何部分相交的概率都是一样的，所以也就不会产生稀疏的参数。</p>
</blockquote>
<h2 id="0x03-Dropout-正则化"><a href="#0x03-Dropout-正则化" class="headerlink" title="0x03 Dropout 正则化"></a>0x03 Dropout 正则化</h2><h3 id="1-Dropout-原理"><a href="#1-Dropout-原理" class="headerlink" title="1. Dropout 原理"></a>1. Dropout 原理</h3><p>Dropout说的简单一点就是：在前向传播的过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃（随机失活），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如下图所示：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Machine%20Learning/image-20210501182040480.png" alt="image-20210501182040480" style="zoom:50%;" />

<blockquote>
<p>注意 dropout 是对某一个隐藏层而言的，如上图就是对三个隐藏层用了三次dropout。</p>
</blockquote>
<h3 id="2-为什么Dropout可以解决过拟合？"><a href="#2-为什么Dropout可以解决过拟合？" class="headerlink" title="2. 为什么Dropout可以解决过拟合？"></a>2. 为什么Dropout可以解决过拟合？</h3><h4 id="（1）取平均的作用："><a href="#（1）取平均的作用：" class="headerlink" title="（1）取平均的作用："></a><strong>（1）取平均的作用</strong>：</h4><p>dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。</p>
<h4 id="（2）减少神经元之间复杂的共适应关系："><a href="#（2）减少神经元之间复杂的共适应关系：" class="headerlink" title="（2）减少神经元之间复杂的共适应关系："></a><strong>（2）减少神经元之间复杂的共适应关系</strong>：</h4><p>因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样<strong>权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况</strong> 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。</p>
<blockquote>
<p>换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些<strong>共同的特征</strong>。</p>
</blockquote>
<h2 id="0x04-参考文章"><a href="#0x04-参考文章" class="headerlink" title="0x04 参考文章"></a>0x04 参考文章</h2><p>[1] <a target="_blank" rel="noopener" href="https://www.cnblogs.com/jianxinzhou/p/4083921.html">机器学习之正则化（Regularization）</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/liuweiyuxiang/article/details/99984288">正则化的作用以及L1和L2正则化的区别</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://blog.csdn.net/lqz790192593/article/details/83045114">过拟合以及解决办法</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://blog.csdn.net/red_stone1/article/details/80755144?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0&spm=1001.2101.3001.4242">【通俗易懂】机器学习中 L1 和 L2 正则化的直观解释</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://blog.csdn.net/program_developer/article/details/80737724">深度学习中Dropout原理解析</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jay</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://jay1zhang.github.io/2021/05/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/">http://jay1zhang.github.io/2021/05/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="http://jayyy1.gitee.io/images/pictures/cartoons/014.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/03/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%B8%B8%E8%A7%81normalization%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94/"><img class="prev-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/016.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">「深度学习」常见normalization方法原理及对比</div></div></a></div><div class="next-post pull-right"><a href="/2021/04/29/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B%EF%BC%9A%E4%BB%8E%20LeNet5%20%E5%88%B0%20DenseNet%20/"><img class="next-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/010.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">「深度学习」卷积网络架构的演进：从 LeNet5 到 DenseNet</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/03/27/Computer Science/Machine Learning/「机器学习」最优化问题之拉格朗日对偶性/" title="「机器学习」（TODO）最优化问题之拉格朗日乘子法"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/038.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-27</div><div class="title">「机器学习」（TODO）最优化问题之拉格朗日乘子法</div></div></a></div><div><a href="/2021/06/01/Computer Science/Machine Learning/「机器学习」面试常见问题汇总/" title="「机器学习」面试常见问题汇总"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/030.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-01</div><div class="title">「机器学习」面试常见问题汇总</div></div></a></div><div><a href="/2022/01/06/Computer Science/Machine Learning/「机器学习」《机器学习》笔记/" title="「机器学习」《机器学习》笔记"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/039.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-06</div><div class="title">「机器学习」《机器学习》笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0x00-%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-text">0x00 过拟合问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x01-%E5%8A%A0%E5%85%A5%E6%AD%A3%E5%88%99%E9%A1%B9"><span class="toc-text">0x01 加入正则项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x02-L1%E3%80%81L2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">0x02 L1、L2 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-L1%E3%80%81L2-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">1. L1、L2 正则化的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-L1%E3%80%81L2-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="toc-text">2. L1、L2 正则化的直观解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0-lambda"><span class="toc-text">3. 正则化参数 $\lambda$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-L1%E3%80%81L2-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-text">4. L1、L2 正则化的特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x03-Dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">0x03 Dropout 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Dropout-%E5%8E%9F%E7%90%86"><span class="toc-text">1. Dropout 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88Dropout%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="toc-text">2. 为什么Dropout可以解决过拟合？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%8F%96%E5%B9%B3%E5%9D%87%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%9A"><span class="toc-text">（1）取平均的作用：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%87%8F%E5%B0%91%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B9%8B%E9%97%B4%E5%A4%8D%E6%9D%82%E7%9A%84%E5%85%B1%E9%80%82%E5%BA%94%E5%85%B3%E7%B3%BB%EF%BC%9A"><span class="toc-text">（2）减少神经元之间复杂的共适应关系：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x04-%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0"><span class="toc-text">0x04 参考文章</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/012.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Jay</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>