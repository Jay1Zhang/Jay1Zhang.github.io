<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>「深度学习」常见normalization方法原理及对比 | J1z's Blog</title><meta name="keywords" content="Deep Learning"><meta name="author" content="Jay"><meta name="copyright" content="Jay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="0x00 标准化与归一化 这一节与本文要重点介绍的 normalization 无关，只是为了区别这两个概念。  1. 关于二者在概念上的勘误网上各种资料，抄来抄去，而在于大家讨论的概念内涵不统一，导致 “标准化” (standardization) 和 “归一化” (normalization) 这两个词长期被混用并被传播。 根据维基百科 Feature scaling - Wikipedia">
<meta property="og:type" content="article">
<meta property="og:title" content="「深度学习」常见normalization方法原理及对比">
<meta property="og:url" content="http://jay1zhang.github.io/2021/05/03/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%B8%B8%E8%A7%81normalization%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94/index.html">
<meta property="og:site_name" content="J1z&#39;s Blog">
<meta property="og:description" content="0x00 标准化与归一化 这一节与本文要重点介绍的 normalization 无关，只是为了区别这两个概念。  1. 关于二者在概念上的勘误网上各种资料，抄来抄去，而在于大家讨论的概念内涵不统一，导致 “标准化” (standardization) 和 “归一化” (normalization) 这两个词长期被混用并被传播。 根据维基百科 Feature scaling - Wikipedia">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/028.jpg">
<meta property="article:published_time" content="2021-05-03T01:13:25.000Z">
<meta property="article:modified_time" content="2021-05-04T09:30:26.827Z">
<meta property="article:author" content="Jay">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/028.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://jay1zhang.github.io/2021/05/03/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%B8%B8%E8%A7%81normalization%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"WKEK6XV2V5","apiKey":"e093fbb9f29fdf5c7ddd56ec43e9ae05","indexName":"Jay","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-04 17:30:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">51</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/028.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">J1z's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">「深度学习」常见normalization方法原理及对比</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-05-03T01:13:25.000Z" title="Created 2021-05-03 09:13:25">2021-05-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-05-04T09:30:26.827Z" title="Updated 2021-05-04 17:30:26">2021-05-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/">Computer Science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/Deep-Learning/">Deep Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">4.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>13min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="0x00-标准化与归一化"><a href="#0x00-标准化与归一化" class="headerlink" title="0x00 标准化与归一化"></a>0x00 标准化与归一化</h2><blockquote>
<p>这一节与本文要重点介绍的 normalization 无关，只是为了区别这两个概念。</p>
</blockquote>
<h3 id="1-关于二者在概念上的勘误"><a href="#1-关于二者在概念上的勘误" class="headerlink" title="1. 关于二者在概念上的勘误"></a>1. 关于二者在概念上的勘误</h3><p>网上各种资料，抄来抄去，而在于大家讨论的概念内涵不统一，导致 “标准化” (standardization) 和 “归一化” (normalization) 这两个词长期被混用并被传播。</p>
<p>根据维基百科 <a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Feature_scaling">Feature scaling - Wikipedia</a> ，”标准化”和”归一化”这两个中文词指代的是四种 Feature scaling （特征缩放）方法：</p>
<ol>
<li><p>Rescaling (min-max normalization) :</p>
<p>$$x’ = \frac{x - min(x)}{max(x) - min(x)}$$</p>
</li>
<li><p>Mean normalization :</p>
<p>$$x’ = \frac{x - mean(x)}{max(x) - min(x)}$$</p>
</li>
<li><p>Standardization (Z-score normalization) :</p>
<p>$$x’ = \frac{x - \bar{x} }{\sigma}$$</p>
</li>
<li><p>Scaling to unit length :</p>
<p>$$x’ = \frac{x}{||x||}$$</p>
</li>
</ol>
<p>也就是说，<strong>标准化实际上是的一种方式，标准化（standardization）也叫做 Z-score 归一化（Z-score Normalization）</strong>。</p>
<p>因而我们可以认为，归一化和标准化都是对数据做变换的方式，能够将原始的一列数据转换到某个范围，或者某种形态，具体的：</p>
<ul>
<li><p><strong>归一化（Normalization）</strong>：将数据变换到某个固定区间（范围）中。通常情况下，这个区间是[0, 1]。</p>
<p>$$x’ = \frac{x - min(x)}{max(x) - min(x)}$$</p>
<blockquote>
<p>广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]；</p>
</blockquote>
</li>
<li><p><strong>标准化（Standardization）</strong>：将数据变换为均值为0，标准差为1的分布。</p>
<p>$$x’ = \frac{x - \bar{x} }{\sigma}$$</p>
<blockquote>
<p>切记，标准化后的数据分布并非一定是正态的。因为标准化只是把数据的均值和方差缩放到了0和1，但并不改变原始数据的分布，更不可能直接就成了标准正态分布，除非原始数据就遵循正态分布。</p>
</blockquote>
</li>
<li><p><strong>中心化</strong>：另外，还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。</p>
</li>
</ul>
<h3 id="2-二者的区别与联系"><a href="#2-二者的区别与联系" class="headerlink" title="2. 二者的区别与联系"></a>2. 二者的区别与联系</h3><p>本质上说，标准化和归一化都是对数据的线性变换。</p>
<p>区别在于，归一化会使得那些扁平分布的数据伸缩变换成类圆形，这就改变了原始数据的分布。而标准化把特征的各个维度标准化到特定的区间，使得不同度量之间的特征具有可比性，同时不改变原始数据的分布。</p>
<p>我们后文中提到的 Normalization ，实际上均为 Z-score Normalization ，即标准化操作，注意不要混淆即可。</p>
<h2 id="0x01-为什么需要-Normalization-？"><a href="#0x01-为什么需要-Normalization-？" class="headerlink" title="0x01 为什么需要 Normalization ？"></a>0x01 为什么需要 Normalization ？</h2><h3 id="1-深度学习中的-Internal-Covariate-Shift-问题及其影响"><a href="#1-深度学习中的-Internal-Covariate-Shift-问题及其影响" class="headerlink" title="1. 深度学习中的 Internal Covariate Shift 问题及其影响"></a>1. 深度学习中的 Internal Covariate Shift 问题及其影响</h3><p>深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。</p>
<p>为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>Google 将这一现象总结为 Internal Covariate Shift，简称 ICS.</p>
<p>那 ICS 会导致什么问题？</p>
<ul>
<li>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</li>
<li>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</li>
<li>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li>
</ul>
<h3 id="2-Normalization-的目的"><a href="#2-Normalization-的目的" class="headerlink" title="2. Normalization 的目的"></a>2. Normalization 的目的</h3><p>为了解决 ICS 问题，在数据分析之前，我们通常需要先将数据标准化，利用标准化后的数据进行数据分析。</p>
<p>数据标准化处理主要包括<strong>数据同趋化处理</strong>和<strong>无量纲化处理</strong>两个方面：</p>
<ul>
<li><p>数据同趋化处理主要<strong>解决不同性质数据问题</strong>。</p>
<blockquote>
<p>对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标的数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。</p>
</blockquote>
</li>
<li><p>数据无量纲化处理主要<strong>解决数据的可比性问题</strong>。</p>
<blockquote>
<p>经过标准化处理，原始数据均转换为无量纲化指标，即各指标值都处于同一个数量级上，可以进行综合测评分析。</p>
</blockquote>
</li>
</ul>
<p>也就说数据标准化（normalization）处理的目的是：</p>
<ol>
<li><p><strong>把特征的各个维度标准化到特定的区间</strong></p>
</li>
<li><p><strong>把有量纲表达式变为无量纲表达式</strong></p>
</li>
</ol>
<h3 id="3-Normalization-的好处"><a href="#3-Normalization-的好处" class="headerlink" title="3. Normalization 的好处"></a>3. Normalization 的好处</h3><p>数据标准化（normalization）处理有两个好处：</p>
<ol>
<li><p><strong>加快基于梯度下降法模型的收敛速度</strong></p>
<blockquote>
<p>如果特征的各个维度的取值范围不同，那么目标函数的等线很可能是一组椭圆，各个特征的取值范围差别越大，椭圆等高线会更加狭长。由于梯度方向垂直于等高线方向，因而这时<strong>优化路线会较为曲折</strong>，这样迭代会很慢（左图）。</p>
<p>相比之下，如果特征的各个维度取值范围相近，那么目标函数很可能很接近一组于正圆，因而<strong>优化路线就会较为直接</strong>，迭代就会很快（右图）。</p>
</blockquote>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504084951689.png" alt="image-20210504084951689" style="zoom:60%;" />

<p>如上图，x1的取值为0-2000，而x2的取值为1-5，假如只有这两个特征，对其进行优化时，会得到一个窄长的椭圆形，导致在梯度下降时，<strong>梯度的方向为垂直等高线的方向而走之字形路线</strong>，这样会使迭代很慢，相比之下，右图的迭代就很快。</p>
</li>
<li><p><strong>提升模型的精度</strong></p>
<p>在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。</p>
</li>
</ol>
<p>因而，Normalization 十分有必要，它可以让各个特征对结果做出的贡献相同。</p>
<h2 id="0x02-Normalization-究竟在做什么？"><a href="#0x02-Normalization-究竟在做什么？" class="headerlink" title="0x02 Normalization 究竟在做什么？"></a>0x02 Normalization 究竟在做什么？</h2><blockquote>
<p><strong>Normalization 的通用框架与基本思想</strong></p>
</blockquote>
<p>深度神经网络中主要有两类实体：神经元和连接神经元的边，所以按照 normalization 涉及对象的不同可以分为两大类：</p>
<ol>
<li><p>一类是对第 $L$ 层每个神经元的激活值或者说对于第 $L+1$ 层网络神经元的输入值进行 normalization 操作，比如 Batch Norm/Layer Norm/Instance Norm/Group Norm/Switchable Norm 等方法都属于这一类；</p>
</li>
<li><p>另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行 normalization 操作，比如 Weight Norm 就属于这一类。</p>
<blockquote>
<p>广义上讲，机器学习中在损失函数里加入的对参数的L1/L2正则项，也属于第二类标准化操作。</p>
<ul>
<li>L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果；</li>
<li>而L2正则的规范化目标是有效减小原始参数值的大小。</li>
</ul>
</blockquote>
</li>
</ol>
<p>对于第一类 normalization 操作，就像激活函数层、卷积层、全连接层、池化层一样，normalization 也属于网络的一层，作用是将神经元的激活值规整为均值为0，方差为1的标准分布。</p>
<p>即不论哪种 normalization 方法，规范化函数统一都是如下形式：</p>
<p>$$\tau = \frac{a_i - \mu}{\sigma}$$</p>
<p>$$a_i^{norm} = \gamma_i \cdot \tau + \beta_i $$</p>
<p>其中，$\gamma$ 和 $\beta$ 分别被称为缩放和平移变量。</p>
<p>那为什么要引入 $\gamma$ 和 $\beta$ 这两个变量呢？</p>
<blockquote>
<p>打个比方，比如网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，但你强制把数据的均值和标准差归一化处理到0和1，这就把数据分布变换到了s函数的中间部分，相当于这一层网络所学习到的特征分布被破坏了。</p>
</blockquote>
<p>通过引入 $\gamma$ 和 $\beta$ 这两个可学习参数，<strong>保证了每一次数据经过归一化后还能保留原来学习到的特征</strong>，同时又能完成归一化操作。</p>
<p>本文重点介绍<strong>第一类 normalization 操作</strong>。</p>
<h2 id="0x03-常见-Normalization-方法原理及对比"><a href="#0x03-常见-Normalization-方法原理及对比" class="headerlink" title="0x03 常见 Normalization 方法原理及对比"></a>0x03 常见 Normalization 方法原理及对比</h2><p>上一节我们提炼出了 normalization 的通用公式：</p>
<p>$$a_i^{norm} = \gamma_i \cdot \frac{a_i - \mu}{\sigma} + \beta_i$$</p>
<p>在这一节，我们将介绍并对比几个主流的 normalization 方法：Batch Normalization（2015年）、Layer Normalization（2016年）、Instance Normalization（2017年）、Group Normalization（2018年）、Switchable Normalization（2018年）；</p>
<p>如果将输入图像的 shape 记为 <code>[N, C, H, W]</code> ，这几个方法主要的<strong>区别在于归一化所作用的神经元集合 S 不同</strong>。</p>
<blockquote>
<p>为什么这些Normalization需要确定一个神经元集合S呢？</p>
<p>原因很简单，均值和方差是个统计指标，要计算这两个指标一定是在一个集合范围内才可行，这就要求必须指定一个神经元组成的集合，利用这个集合里每个神经元的激活来统计出所需的均值和方差，才能进一步归一化。</p>
</blockquote>
<p>下面我们一一介绍。</p>
<h3 id="1-Batch-Normalization"><a href="#1-Batch-Normalization" class="headerlink" title="1. Batch Normalization"></a>1. Batch Normalization</h3><h4 id="MLP中的BN"><a href="#MLP中的BN" class="headerlink" title="MLP中的BN"></a>MLP中的BN</h4><p>在前向传播网络中，<strong>BN针对的是某个神经元，即输入数据的某一特征维度</strong>，对应 NxE 中的某一列 Nx1。</p>
<p>假设某个batch包含n个sample，那么每个实例在神经元k都会产生一个激活值，BN就是针对这n个激活值进行 normalization 的，如下图：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504154306353.png" alt="image-20210504154306353" style="zoom: 50%;" />



<h4 id="CNN中的BN"><a href="#CNN中的BN" class="headerlink" title="CNN中的BN"></a>CNN中的BN</h4><p>我们知道，CNN中的核心结构为卷积层，假设每个卷积层有m个卷积核，那么输出图像的通道数即为m。对于第 i 个卷积核，它的输入为三维图像 [H, W, C] ，输出激活值为二维平面 [H, W, 1] ，对应输出图像的第 i 个通道，如下图。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504155317244.png" alt="image-20210504155317244" style="zoom:80%;" />



<p>在卷积层中，<strong>BN针对的是某个卷积核，即输入图像的某个通道</strong>。</p>
<p>由于每个实例在通道k都会产生一个二维激活平面 HxW ，那么 n 个实例在这个卷积核将产生 n 个这样的平面，BN 就是针对这 N 个二维平面进行 normalization 的，如下图。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504160324972.png" alt="image-20210504160324972" style="zoom:80%;" />



<p>看起来似乎有些复杂，但实际上，它和MLP的区别仅在于，MLP中神经元的输入输出都是一个数值，而CNN中卷积核的输入输出都是一个二维平面。</p>
<h4 id="BN公式"><a href="#BN公式" class="headerlink" title="BN公式"></a>BN公式</h4><p>BN针对的是某一隐藏层的某个神经元来说的，假设一个 batch 有 m 个实例，那我们就在这 m 个实例上做如下过程：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504154419708.png" alt="image-20210504154419708" style="zoom:75%;" />

<p>实际上，和前面抽象出的通用公式别无二异。</p>
<h4 id="BN-存在的问题"><a href="#BN-存在的问题" class="headerlink" title="BN 存在的问题"></a>BN 存在的问题</h4><ol>
<li>对 batch size 大小比较敏感。如果size太小，计算的均值、方差不足以代表整个数据分布，会大大影响结果。</li>
<li>在图片风格转换等应用场景，使用BN会带来负面效果。这很可能是因为BN在 Batch 内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。</li>
<li>RNN等动态网络使用BN效果不佳且使用起来不方便。这是因为对于RNN来说，输入的Sequence序列是不定长的，这会使的BN得到的size不稳定，结果可信度下降。</li>
</ol>
<p>上面所列BN的缺点，其实深入思考，都指向了幕后同一个黑手：</p>
<ul>
<li>即BN要求计算统计量的时候必须在同一个 Batch 内的实例之间进行统计，这就形成了Batch内实例之间的相互依赖和影响的关系。</li>
</ul>
<p>那如何从根本上解决这些问题？一个自然的想法是：<strong>转换统计集合范围</strong>。</p>
<ul>
<li>在统计均值方差的时候，不依赖Batch内数据，<strong>只用当前处理的单个训练数据来获得均值方差的统计量</strong>，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。</li>
</ul>
<h3 id="2-Layer-Normalization"><a href="#2-Layer-Normalization" class="headerlink" title="2. Layer Normalization"></a>2. Layer Normalization</h3><p>为了解决BN对size敏感的问题，我们完全可以直接用同一隐藏层的神经元的响应值作为集合S的范围来求均值和方差，这就是Layer Normalization的基本思想。</p>
<h4 id="MLP中的LN"><a href="#MLP中的LN" class="headerlink" title="MLP中的LN"></a>MLP中的LN</h4><p>在前向传播网络中，<strong>LN针对的是某个实例的所有特征</strong>，即 NxE 中的某一行 1xE 。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504162120954.png" alt="image-20210504162120954" style="zoom:80%;" />



<h4 id="CNN中的LN"><a href="#CNN中的LN" class="headerlink" title="CNN中的LN"></a>CNN中的LN</h4><p>在卷积层中，<strong>LN针对的是该层的所有卷积核，即输入图像的所有通道</strong>。</p>
<p>对每个实例来说，n个卷积核将产生n个通道，即n个二维激活屏幕 HxW，BN 就是针对这 n 个二维平面进行 normalization 的，如下图。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504162711011.png" alt="image-20210504162711011" style="zoom:80%;" />



<h4 id="RNN中的LN"><a href="#RNN中的LN" class="headerlink" title="RNN中的LN"></a>RNN中的LN</h4><p>不再赘述。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504164047746.png" alt="image-20210504164047746" style="zoom:80%;" />



<h4 id="LN公式"><a href="#LN公式" class="headerlink" title="LN公式"></a>LN公式</h4><p>LN针对的是<strong>某一隐藏层的所有神经元的输入</strong>，假设有 H 个神经元（对应H个特征或H个通道），按以下公式进行 normalization 操作：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504163955731.png" alt="image-20210504163955731" style="zoom:80%;" />



<h4 id="LN与BN的区别"><a href="#LN与BN的区别" class="headerlink" title="LN与BN的区别"></a>LN与BN的区别</h4><ul>
<li><p>Batch Normalization  是<strong>以batch为单位计算归一化统计量</strong>；</p>
</li>
<li><p>Layer Normalization 则是<strong>对同一个样本的所有特征做归一化统计量</strong>，它是一个独立于batch size的算法，针对网络中某一层的所有神经元的输入进行归一。</p>
</li>
</ul>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210419093940324.png" alt="image-20210419093940324" style="zoom:60%;" />



<h3 id="3-Instance-Normalization"><a href="#3-Instance-Normalization" class="headerlink" title="3. Instance Normalization"></a>3. Instance Normalization</h3><p>Layer Normalization 为了能够抛开对 Mini-Batch 的依赖，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN来说是显而易见的。</p>
<p>因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到<strong>单个卷积核对应的输出通道内部</strong>。</p>
<h4 id="CNN中的IN"><a href="#CNN中的IN" class="headerlink" title="CNN中的IN"></a>CNN中的IN</h4><p>下图展示了CNN中的 Instance Normalization。</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504164455797.png" alt="image-20210504164455797" style="zoom:80%;" />



<p>对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。</p>
<blockquote>
<p>对于RNN或者MLP，如果在同一个隐藏层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。</p>
</blockquote>
<h4 id="IN公式"><a href="#IN公式" class="headerlink" title="IN公式"></a>IN公式</h4><img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504170742822.png" alt="image-20210504170742822" style="zoom:80%;" />

<p>其中，[t, i, j, k] 分别代表 [N, C, H, W] ，$\mu_{ti}$ 与 $\sigma_{ti}^2$ 均代表一个二维矩阵。</p>
<h4 id="IN与BN的区别"><a href="#IN与BN的区别" class="headerlink" title="IN与BN的区别"></a>IN与BN的区别</h4><p>我们回想下CNN中的BN，针对的是相同通道进行 normalization。也就是说，看上去，IN 就像是 BN 的一种 <code>batch size = 1</code> 的特例情况。</p>
<ul>
<li>BN适用于判别模型中，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。</li>
<li>IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。</li>
</ul>
<h3 id="4-Group-Normalization"><a href="#4-Group-Normalization" class="headerlink" title="4. Group Normalization"></a>4. Group Normalization</h3><p>从前面的 Layer Normalization 和 Instance Normalization 的过程可以看出，这是两种极端情况：</p>
<ul>
<li>LN 是将<strong>同一层所有神经元作为统计范围</strong>；</li>
<li>IN 则是将同一卷积层中<strong>每个卷积核对应的输出通道单独作为自己的统计范围</strong>。</li>
</ul>
<p>那么，有没有介于两者之间的统计范围呢？</p>
<p>我们自然而然会想到<strong>对某一卷积层的所有输出通道进行分组，并在分组范围内分别进行统计</strong>，这就是Group Normalization的核心思想。</p>
<h4 id="CNN中的GN"><a href="#CNN中的GN" class="headerlink" title="CNN中的GN"></a>CNN中的GN</h4><p>下图展示了CNN中的 Group Normalization。</p>
<p><img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504170441339.png" alt="image-20210504170441339"></p>
<h4 id="GN公式"><a href="#GN公式" class="headerlink" title="GN公式"></a>GN公式</h4><img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504170941018.png" alt="image-20210504170941018" style="zoom:80%;" />





<p>Group Normalization在要求Batch Size比较小的场景下或者物体检测／视频分类等应用场景下效果是优于BN的。</p>
<h3 id="第一类-Normalization-方法对比"><a href="#第一类-Normalization-方法对比" class="headerlink" title="第一类 Normalization 方法对比"></a>第一类 Normalization 方法对比</h3><p>我们将输入图像的 shape 记为 [N, C, H, W]，这四种方法的区别如下图所示：</p>
<img src="https://gitee.com/Jayyy1/images/raw/master/posts/Computer%20Science/Deep%20Learning/image-20210504171454959.png" alt="image-20210504171454959" style="zoom:80%;" />

<blockquote>
<p>为了方便表示，我们将 H,W 压缩到一维，上图中一个竖列代表一张图片。</p>
</blockquote>
<ul>
<li>Batch Norm：在N张图片的相同通道上，对batch进行Normalization，共进行C次。</li>
<li>Layer Norm：在一张图片的所有通道上，对通道方向进行Normalization，共进行N次。</li>
<li>Instance Norm：在一张图片的一个通道上，对单张图像进行Normalization，共进行NxC次。</li>
<li>Group Norm：将通道分组，并在单张图片的每一组内，对通道方向进行Normalization，共进行Nx~次。</li>
</ul>
<h3 id="5-Weight-Normalization"><a href="#5-Weight-Normalization" class="headerlink" title="5. Weight Normalization"></a>5. Weight Normalization</h3><p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33173246">这篇文章</a>。</p>
<h2 id="0x04-参考文章"><a href="#0x04-参考文章" class="headerlink" title="0x04 参考文章"></a>0x04 参考文章</h2><p>[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/gwplovekimi/article/details/84647354">Normalization</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43200897">深度学习中的Normalization模型</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33173246">详解深度学习中的Normalization，BN/LN/WN</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://www.cnblogs.com/dyl222/p/12197187.html">Batch Normalization和Layer Normalization的对比分析</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://blog.csdn.net/liuxiao214/article/details/81037416">BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jay</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://jay1zhang.github.io/2021/05/03/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%B8%B8%E8%A7%81normalization%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94/">http://jay1zhang.github.io/2021/05/03/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E5%B8%B8%E8%A7%81normalization%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post_share"><div class="social-share" data-image="http://jayyy1.gitee.io/images/pictures/cartoons/028.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/05/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/"><img class="next-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/032.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">「机器学习」过拟合问题与正则化方法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/04/22/Computer Science/Deep Learning/「深度学习」神经网络中的优化算法/" title="「深度学习」神经网络中的优化算法"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/025.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-22</div><div class="title">「深度学习」神经网络中的优化算法</div></div></a></div><div><a href="/2021/04/16/Computer Science/Deep Learning/「深度学习」Attention？Attention！/" title="「深度学习」Attention? Attention!"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/028.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-16</div><div class="title">「深度学习」Attention? Attention!</div></div></a></div><div><a href="/2021/04/17/Computer Science/Deep Learning/「深度学习」Transformer 模型详解/" title="「深度学习」Transformer 模型详解"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/006.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-17</div><div class="title">「深度学习」Transformer 模型详解</div></div></a></div><div><a href="/2021/04/29/Computer Science/Deep Learning/「深度学习」一文详解卷积神经网络及经典CNN模型/" title="「深度学习」一文详解卷积神经网络及经典CNN模型"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/029.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-29</div><div class="title">「深度学习」一文详解卷积神经网络及经典CNN模型</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0x00-%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">0x00 标准化与归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%85%B3%E4%BA%8E%E4%BA%8C%E8%80%85%E5%9C%A8%E6%A6%82%E5%BF%B5%E4%B8%8A%E7%9A%84%E5%8B%98%E8%AF%AF"><span class="toc-text">1. 关于二者在概念上的勘误</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BA%8C%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="toc-text">2. 二者的区别与联系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x01-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-Normalization-%EF%BC%9F"><span class="toc-text">0x01 为什么需要 Normalization ？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84-Internal-Covariate-Shift-%E9%97%AE%E9%A2%98%E5%8F%8A%E5%85%B6%E5%BD%B1%E5%93%8D"><span class="toc-text">1. 深度学习中的 Internal Covariate Shift 问题及其影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Normalization-%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="toc-text">2. Normalization 的目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Normalization-%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-text">3. Normalization 的好处</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x02-Normalization-%E7%A9%B6%E7%AB%9F%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">0x02 Normalization 究竟在做什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x03-%E5%B8%B8%E8%A7%81-Normalization-%E6%96%B9%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94"><span class="toc-text">0x03 常见 Normalization 方法原理及对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Batch-Normalization"><span class="toc-text">1. Batch Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MLP%E4%B8%AD%E7%9A%84BN"><span class="toc-text">MLP中的BN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84BN"><span class="toc-text">CNN中的BN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN%E5%85%AC%E5%BC%8F"><span class="toc-text">BN公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN-%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">BN 存在的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Layer-Normalization"><span class="toc-text">2. Layer Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MLP%E4%B8%AD%E7%9A%84LN"><span class="toc-text">MLP中的LN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84LN"><span class="toc-text">CNN中的LN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN%E4%B8%AD%E7%9A%84LN"><span class="toc-text">RNN中的LN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LN%E5%85%AC%E5%BC%8F"><span class="toc-text">LN公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LN%E4%B8%8EBN%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">LN与BN的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Instance-Normalization"><span class="toc-text">3. Instance Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84IN"><span class="toc-text">CNN中的IN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IN%E5%85%AC%E5%BC%8F"><span class="toc-text">IN公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IN%E4%B8%8EBN%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">IN与BN的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Group-Normalization"><span class="toc-text">4. Group Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84GN"><span class="toc-text">CNN中的GN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GN%E5%85%AC%E5%BC%8F"><span class="toc-text">GN公式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%B1%BB-Normalization-%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-text">第一类 Normalization 方法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Weight-Normalization"><span class="toc-text">5. Weight Normalization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x04-%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0"><span class="toc-text">0x04 参考文章</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/012.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jay</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>