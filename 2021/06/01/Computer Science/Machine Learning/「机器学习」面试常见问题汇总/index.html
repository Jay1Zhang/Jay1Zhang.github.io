<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>「机器学习」面试常见问题汇总 | J1z's Blog</title><meta name="keywords" content="Machine Learning"><meta name="author" content="Jay"><meta name="copyright" content="Jay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="机器学习基础验证集与测试集 验证集的作用： 使用验证集是为了 快速调参，也就是用验证集选择超参数（网络层数，网络节点数，迭代次数，学习率这些）。另外用验证集还可以监控模型是否异常（过拟合啦什么的），然后决定是不是要提前停止训练。 验证集的关键在于 选择超参数，我们手动调参是为了让模型在验证集上的表现越来越好，如果把测试集作为验证集，调参去拟合测试集，就有点像作弊了。  而测试集既 不参与参数的学习">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习」面试常见问题汇总">
<meta property="og:url" content="http://jay1zhang.github.io/2021/06/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/index.html">
<meta property="og:site_name" content="J1z&#39;s Blog">
<meta property="og:description" content="机器学习基础验证集与测试集 验证集的作用： 使用验证集是为了 快速调参，也就是用验证集选择超参数（网络层数，网络节点数，迭代次数，学习率这些）。另外用验证集还可以监控模型是否异常（过拟合啦什么的），然后决定是不是要提前停止训练。 验证集的关键在于 选择超参数，我们手动调参是为了让模型在验证集上的表现越来越好，如果把测试集作为验证集，调参去拟合测试集，就有点像作弊了。  而测试集既 不参与参数的学习">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/030.jpg">
<meta property="article:published_time" content="2021-06-01T00:54:33.000Z">
<meta property="article:modified_time" content="2021-08-04T05:03:47.006Z">
<meta property="article:author" content="Jay">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/030.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://jay1zhang.github.io/2021/06/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"WKEK6XV2V5","apiKey":"e093fbb9f29fdf5c7ddd56ec43e9ae05","indexName":"Jay","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-08-04 13:03:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">57</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/030.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">J1z's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">「机器学习」面试常见问题汇总</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-06-01T00:54:33.000Z" title="Created 2021-06-01 08:54:33">2021-06-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-08-04T05:03:47.006Z" title="Updated 2021-08-04 13:03:47">2021-08-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/">Computer Science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>16min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><h3 id="验证集与测试集"><a href="#验证集与测试集" class="headerlink" title="验证集与测试集"></a>验证集与测试集</h3><ul>
<li><p>验证集的作用：</p>
<p>使用验证集是为了 <strong>快速调参</strong>，也就是用验证集选择超参数（网络层数，网络节点数，迭代次数，学习率这些）。另外用验证集还可以监控模型是否异常（过拟合啦什么的），然后决定是不是要提前停止训练。</p>
<p>验证集的关键在于 <strong>选择超参数</strong>，我们手动调参是为了让模型在验证集上的表现越来越好，如果把测试集作为验证集，调参去拟合测试集，就有点像作弊了。</p>
</li>
<li><p>而测试集既 <strong>不参与参数的学习过程</strong>，也 <strong>不参与参数的选择过程</strong>，仅仅用于模型评价。</p>
</li>
</ul>
<h3 id="Bias-与-Variance"><a href="#Bias-与-Variance" class="headerlink" title="Bias 与 Variance"></a>Bias 与 Variance</h3><p>欠拟合也称为高偏差（bias），过拟合也称为高方差（variance）。</p>
<p>关于过拟合问题，要注意：</p>
<blockquote>
<p>Batch Normalization的主要作用是加快网络的训练速度，一般不说它是防止过拟合的。</p>
<p>如果硬要说是防止过拟合，可以这样理解：</p>
<p>BN每次的mini-batch的数据都不一样，但是每次的mini-batch的数据都会对moving mean和moving variance产生作用，可以认为是引入了噪声，这就可以认为是进行了data augmentation，而data augmentation被认为是防止过拟合的一种方法。因此，可以认为用BN可以防止过拟合。</p>
</blockquote>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>神经网络的核心思想就是通过调整参数w和b，来尽可能地最小化损失函数，解决这个问题的方法就是梯度下降法。</p>
<ul>
<li><p>直观理解：</p>
<blockquote>
<p>⾸先把我们的函数想象成⼀个⼭⾕。我们想象有⼀个⼩球从⼭⾕的斜坡滚落下来。我们的⽇常经验告诉我们这个球最终会沿着最陡峭的方向滚到⾕底。</p>
<p>我们会为⼀个（假想的）球体随机选择⼀个起始位置，然后模拟球体滚落到⾕底的运动。这其实就是梯度下降法的核心思想。我们可以通过计算 <em>C</em> 的导数（或者⼆阶导数）来简单模拟——<strong>这些导数会告诉我们⼭⾕中局部“形状”</strong>的⼀切，<strong>由此知道我们的球将怎样滚动</strong>。</p>
<p>我们知道梯度指向的是某一点处方向导数最大的方向，也就是最陡峭的方向。</p>
</blockquote>
</li>
</ul>
<p>对于凸优化问题，梯度下降法一定能找到全局最优点，但非凸函数则不一定，可能会陷入局部最优。</p>
<h3 id="极大似然估计与贝叶斯估计"><a href="#极大似然估计与贝叶斯估计" class="headerlink" title="极大似然估计与贝叶斯估计"></a>极大似然估计与贝叶斯估计</h3><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><ul>
<li><strong>全概率公式：</strong>设事件 <img src="https://www.zhihu.com/equation?tex=B_%7B1%7D,B_%7B2%7D,...B_%7Bn%7D" alt="[公式]"> 构成一个完备事件组，即它们两两不相容，和为全集且 <img src="https://www.zhihu.com/equation?tex=P(B_%7Bi%7D)%3E0" alt="[公式]"> ，则对任一事件 <img src="https://www.zhihu.com/equation?tex=A" alt="[公式]"> 有：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=P(A)=%5CSigma_%7Bi=1%7D%5E%7Bn%7DP(B_%7Bi%7D)P(A%7CB_%7Bi%7D)" alt="[公式]"></p>
<p>​        可以看出，全概率公式是“由因推果”的思想，当知道某件事的原因后，推断由某个原因导致这件事发生的概率为多少。</p>
<ul>
<li><strong>贝叶斯公式：</strong>符号定义与全概率公式相同，则：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=P(B_%7Bi%7D%7CA)=%5Cfrac%7BP(B_%7Bi%7D)P(A%7CB_%7Bi%7D)%7D%7BP(A)%7D+=%5Cfrac%7BP(B_%7Bi%7D)P(A%7CB_%7Bi%7D)%7D%7B%5CSigma_%7Bi=1%7D%5E%7Bn%7DP(B_%7Bi%7D)P(A%7CB_%7Bi%7D)%7D" alt="[公式]"></p>
<p>​        可以看出，贝叶斯公式是“由果溯因”的思想，当知道某件事的结果后，由结果推断这件事是由各个原因导致的概率为多少。</p>
<ul>
<li><p><strong>先验概率（prior probability）：</strong>指根据以往经验和分析，在实验或采样前就可以得到的概率，如上式中的 $P(B_i)$。</p>
</li>
<li><p><strong>后验概率（posterior probability）：</strong>指某件事 $A$ 已经发生，想要计算这件事发生的原因是由某个因素引起的概率。</p>
<p>可以看出，先验概率就是事先可估计的概率分布，而后验概率类似贝叶斯公式“由果溯因”的思想。</p>
</li>
</ul>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55791843">这篇文章</a>。</p>
<ul>
<li><p><strong>似然函数</strong>：</p>
<p>似然性（likelihood）与概率（possibility）同样可以表示事件发生的可能性大小，但是二者有着很大的区别：</p>
<ul>
<li>条件概率 <img src="https://www.zhihu.com/equation?tex=p(x%7C%5Ctheta)" alt="[公式]"> 是在已知参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 的情况下，发生观测结果 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 可能性大小；</li>
<li>似然函数 <img src="https://www.zhihu.com/equation?tex=L(%5Ctheta%7Cx)" alt="[公式]"> 则是从观测结果 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 出发，分布函数的参数为 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 的可能性大小；</li>
</ul>
<blockquote>
<p>这里可能不是那么好理解，我们再详细说明下，似然函数如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=L(%5Ctheta%7Cx)=p(x%7C%5Ctheta)%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 已知， <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 未知。若对于两个参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_1" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=%5Ctheta_2" alt="[公式]"> ，有</p>
<p><img src="https://www.zhihu.com/equation?tex=L(%5Ctheta_1%7Cx)=p(x%7C%5Ctheta_1)%3Ep(x%7C%5Ctheta_2)=L(%5Ctheta_2%7Cx)%5C%5C" alt="[公式]"></p>
<p>那么意味着当 <img src="https://www.zhihu.com/equation?tex=%5Ctheta=%5Ctheta_1" alt="[公式]"> 时，随机变量 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> 生成 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的概率大于当参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta=%5Ctheta_2" alt="[公式]"> 时。</p>
<p>这也正是似然的意义所在，若观测数据为 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，那么 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_1" alt="[公式]"> 是比 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_2" alt="[公式]"> 更有可能为分布函数的参数。</p>
</blockquote>
</li>
<li><p><strong>最大似然估计</strong>：</p>
<ul>
<li><p>对于给定样本值，使似然函数 $L$ 达到最大值的参数值 $\theta^*$ 称为未知参数 $\theta$ 的最大似然估计值。</p>
</li>
<li><p>最大似然估计的思想在于，对于给定的观测数据 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，我们希望能从所有的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_1,%5Ctheta_2,%5Ccdots,%5Ctheta_n" alt="[公式]"> 中找出<strong>能最大概率生成观测数据</strong>的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E*" alt="[公式]"> 作为估计结果，因此被估计出的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E*" alt="[公式]"> 应该满足：</p>
</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E*=arg%5Cmax_%7B%5Ctheta%7Dp(x%7C%5Ctheta)%5C%5C" alt="[公式]"></p>
<ul>
<li>最大化的步骤是通过对未知参数求偏导（即求极值），并令其等于0来解得。</li>
</ul>
</li>
</ul>
<h4 id="二者的区别"><a href="#二者的区别" class="headerlink" title="二者的区别"></a>二者的区别</h4><p>从贝叶斯公式的数学表达就可以看到，贝叶斯估计引入了先验概率，通过先验概率与似然概率来求解后验概率，而最大似然估计是直接通过最大化似然函数（条件概率）来求解得出的。</p>
<ul>
<li><p>我们举例来说明：</p>
<p>假设某人感染了新冠病毒，那么其核酸检测呈阳性的概率为 95%；如果未感染病毒，则阳性的概率为 1%。现在有一个人结果为阳性，问这个人感染病毒了吗？</p>
</li>
<li><p>极大似然估计：</p>
<p>既然感染了病毒出现阳性的概率为95%，没感染出现阳性的概率只有1%，本着谁大像谁的原则，那我们认为这个人已经感染了病毒。</p>
</li>
<li><p>贝叶斯估计：</p>
<p>首先我们必须知道先验概率，即整个人群中感染此病毒的人数为10%左右，那么由贝叶斯公式：</p>
<p>$$P(感染|核酸检测阳性) = \frac{P(核酸检测为阳性且感染了新冠病毒)}{P(核酸检测阳性)} = \frac{P(感染) P(阳性|感染)}{P(感染) P(阳性|感染) + P(未感染) P(阳性|未感染)} = \frac{0.1 \times 0.95}{0.1 \times 0.95 + 0.9 \times 0.01} = 0.51$$ </p>
<p>即当检测呈阳性时，几乎是只有一半概率能确信其感染了新冠病毒（只是随便举的数据）。</p>
</li>
</ul>
<p>从本质上来说，最大似然是对点估计，贝叶斯推断是对分布估计。即最大似然是求出最有可能的 $\theta$ 值，而贝叶斯推断则是求解 $\theta$ 的分布。</p>
<h4 id="二者的联系"><a href="#二者的联系" class="headerlink" title="二者的联系"></a>二者的联系</h4><p>当先验分布是均匀分布时，取后验概率最大，就能从贝叶斯估计得到极大似然估计，即二者等价：</p>
<p>$$\hat{P}(\theta | D) = \frac{P(\theta)P(D|\theta)}{P(D)} = \frac{P(\theta)P(D|\theta)}{\sum_i P(\theta_i)P(D|\theta_i)} = \frac{P(D|\theta)}{\sum_i P(D|\theta_i)} = \frac{P(D|\theta)}{C}$$</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55793850">这里</a> 。</p>
</blockquote>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>最小二乘法也被称作最小平方法，是一种求解矩阵方程参数的算法。</p>
<p>假设对于一个二维空间中，有这么一些点是呈现线性关系的：</p>
<img src="https://pic1.zhimg.com/v2-bf24d89bcbbc276129f8134c24d72580_r.jpg" alt="img" style="zoom:15%;" />

<p>我们希望用一个线性模型（函数） <img src="https://www.zhihu.com/equation?tex=y=ax+b" alt="[公式]"> 表示上面的关系。最小二乘法就是<strong>试图找到一条直线（即确定a，b），使得所有样本到直线上的欧式距离之和最小</strong>，亦即误差最小：</p>
<p><img src="https://www.zhihu.com/equation?tex=a%5E*,b%5E*=arg%5Cmin_%7Ba,b%7D%5Csum_%7Bi=1%7D%5En%5Cleft(y_i-ax_i-b%5Cright)%5E2%5C%5C" alt="[公式]"></p>
<p>这便是最小二乘法了。为什么叫二乘法？因为是最小化平方和。</p>
<h3 id="求解步骤"><a href="#求解步骤" class="headerlink" title="求解步骤"></a>求解步骤</h3><p>最小二乘法的步骤：对 $\theta$ 求偏导，让偏导等于0，求出 $\theta$ 值。</p>
<h3 id="最小二乘法与最大似然估计的区别与联系"><a href="#最小二乘法与最大似然估计的区别与联系" class="headerlink" title="最小二乘法与最大似然估计的区别与联系"></a>最小二乘法与最大似然估计的区别与联系</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20447622/answer/256130452">最大似然估计和最小二乘法怎么理解？</a></p>
</blockquote>
<ul>
<li>最小二乘法要在很多条线中间选择出一条距离所有的样本点的欧氏距离之和最小的，即误差最小的直线。</li>
<li>最大似然估计是选择一个<strong>能最大概率生成观测数据</strong>的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E*" alt="[公式]"> 作为估计结果；做法是把我们观察到每个样本的概率乘到一起，然后试图调整参数以最大化这个概率的乘积。</li>
</ul>
<h2 id="关于损失函数"><a href="#关于损失函数" class="headerlink" title="关于损失函数"></a>关于损失函数</h2><h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/b1055077005/article/details/100152102">交叉熵损失函数原理详解</a></p>
<p>交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性，要理解交叉熵，需要先了解下面几个概念。</p>
<h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>信息奠基人香农（Shannon）认为“信息是用来消除随机不确定性的东西”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。</p>
<ul>
<li><p>“<strong>太阳从东边升起</strong>”，这条信息并没有减少不确定性，因为太阳肯定是从东边升起的，这是一句废话，信息量为0。</p>
</li>
<li><p>”<strong>2018年中国队成功进入世界杯</strong>“，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，而这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。</p>
</li>
</ul>
<p>根据上述可总结如下：<strong>信息量的大小与信息发生的概率成反比</strong>。概率越大，信息量越小。概率越小，信息量越大。</p>
<p>设某一事件发生的概率为P(x)，其信息量表示为：</p>
<p>$$I(x) = - log(P(x))$$</p>
<p>其中，$I(x)$ 表示信息量，log表示以e为底的自然对数。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>信息熵也被称为熵，用来表示所有信息量的期望。期望是试验中每次可能结果的概率乘以其结果的总和。</p>
<p>所以信息量的熵可表示为：</p>
<p>$$H(X) = - \sum^n_{i=1} P(x_i) log(P(x_i))$$</p>
<p>其中， $X$ 是一个离散型随机变量，且有 $X = x_1, x_2, \dots, x_n$</p>
<h4 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h4><p>如果对于同一个随机变量 $X$ 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，则我们可以使用KL散度来衡量这两个概率分布之间的差异。</p>
<p>下面直接列出公式，再举例子加以说明。</p>
<p>$$D_{KL} (p || q) = \sum^n_{i=1} p(x_i) log(\frac{p(x_i)}{q(x_i)})$$</p>
<p>在机器学习中，常常使用 $P\left(x\right)$ 来表示样本的真实分布，$Q \left(x\right)$ 来表示模型所预测的分布。KL散度越小，表示 $P(x)$ 与 $Q(x)$ 的分布越接近，可以通过反复训练迭代来使 $Q(x)$ 的分布更加接近真实分布 $P(x)$ 。 </p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>首先将KL散度公式拆开：</p>
<p>$$D_{KL} (p || q) = \sum^n_{i=1} p(x_i) log(\frac{p(x_i)}{q(x_i)}) \ = \sum^n_{i=1} p(x_i) log(p(x_i)) - \sum^n_{i=1} p(x_i) log(q(x_i)) \ = - H(p(x)) + [-\sum^n_{i=1} p(x_i) log(q(x_i))] $$</p>
<p>前者 $H \left (p \left (x \right)\right)$ 表示信息熵，后者即为交叉熵，<strong>KL散度 = 交叉熵 - 信息熵</strong>。</p>
<p>所以交叉熵公式表示为：</p>
<p>$$H(p, q) = -\sum^n_{i=1} p(x_i) log(q(x_i))$$</p>
<p>在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布 $P\left(x \right)$ 也就确定下来了，所以<strong>信息熵在这里就是一个常量</strong>。由于KL散度的值表示真实概率分布 $P\left(x\right)$ 与预测概率分布 $Q \left(x\right)$ 之间的差异，值越小表示预测的结果越好，所以需要最小化KL散度，而<strong>交叉熵等于KL散度加上一个常量（信息熵）</strong>，且公式相比KL散度更加容易计算，所以在<strong>机器学习中常常使用交叉熵损失函数来计算loss就行了</strong>。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。</li>
<li>交叉熵在分类问题中常常与softmax是标配（<code>tf.cross_entropy_with_softmax</code>），首先利用softmax处理输出的结果，使其多个分类的预测值和为1，再通过交叉熵来计算损失。</li>
</ul>
<h3 id="MAE与MSE的区别"><a href="#MAE与MSE的区别" class="headerlink" title="MAE与MSE的区别"></a>MAE与MSE的区别</h3><p>6.1号字节二面问到的问题，让我从梯度的方向考虑，当时没答出来。晚上突然意识到自己把这个很重要的公式搞忘掉了：</p>
<p>$$w = w - \eta J(w)$$</p>
<p>我当时以为MAE求导后梯度是个常数值，参数就不会更新了。。。当时脑子可能有点不太清醒。。。</p>
<p>实际上，正由于MAE求导后梯度是个常数值，所以参数每次更新的大小都相同。也就是说，即使损失值很小，梯度也很大，这就不利于模型的学习。</p>
<p>而MSE的梯度，求完导后还是个一阶函数，所以梯度会越来越小，每次更新的大小也会越来越小。</p>
<h2 id="经典算法题"><a href="#经典算法题" class="headerlink" title="经典算法题"></a>经典算法题</h2><h4 id="100层楼扔鸡蛋，确定鸡蛋摔碎的楼层N"><a href="#100层楼扔鸡蛋，确定鸡蛋摔碎的楼层N" class="headerlink" title="100层楼扔鸡蛋，确定鸡蛋摔碎的楼层N"></a>100层楼扔鸡蛋，确定鸡蛋摔碎的楼层N</h4><p><strong>【题目】</strong></p>
<p>某栋大楼有100层。若将鸡蛋从第N层或更高的楼层扔下来，鸡蛋就会摔碎，而从第N层以下的楼层扔下鸡蛋不会摔碎。</p>
<p>现在给你两个鸡蛋，请找出N，并要求最差情况下扔鸡蛋次数最少。</p>
<p><strong>【分析】</strong></p>
<p>刚拿到这个题目我以为是二分法的变式，于是考虑第一次从50层扔鸡蛋，有两种情况：</p>
<ol>
<li>如果鸡蛋摔碎了，说明 N &lt;= 50，且由于你只剩下一个鸡蛋了，所以只能从1~49层开始一层层扔鸡蛋测试，最坏需要49次，加上在50层扔的第一个鸡蛋，最坏需要50次测试；</li>
<li>如果鸡蛋没有碎，说明 N &gt; 50，而且你还有两个鸡蛋，可以接着在51~100层内二分，那这时候最坏情况是多少呢？可以想见，最坏是25次。</li>
</ol>
<p>因此，如果从第50层开始扔，最差情况下需要50次。</p>
<p>那么最好情况呢？就是如果每次测试鸡蛋都没碎，因为每次范围缩小一半，所以最多需要 log100 次，大约是7次。这说明每次缩小一半区间太大了，如果想让最差情况下测试次数最少，就要尽可能地均衡最差情况与最好情况。</p>
<p>所以我们尝试把区间缩小，不妨从第10层开始扔鸡蛋，然后是20层，30层，…，100层：</p>
<ul>
<li><p>如果鸡蛋在第10层就碎了，那么按照上面的分析，最坏情况需要测试10次。</p>
</li>
<li><p>如果鸡蛋在第100层才碎（不可能不碎哈），那说明N在91~100之间，由于在10，20，…,100都扔了鸡蛋，所以“最好”情况为10+9=19次</p>
</li>
</ul>
<p>因此，最差情况为19次，显然我们放缩得太多了，那到底该如何确定每次选择的楼层呢？</p>
<p>按照前面的分析，我们知道，要想最差情况下扔鸡蛋次数最少，需要尽可能地均衡最差情况与最好情况，即让两种情况下的测试次数尽可能相同。</p>
<p><strong>【解法1】</strong></p>
<p>不妨设第一次扔的楼层为x，第二次扔的楼层为y，讨论第一次扔鸡蛋的情况：</p>
<ul>
<li>如果鸡蛋在x层碎了，那么只能挨个测试1~x-1层，共需测试 $x$ 次；</li>
<li>如果鸡蛋在x层没碎，则要在第y层继续测试，此时又会出现两种情况（碎或不碎），但实际上，我们只想要最差情况下需要测试几次，而最差情况肯定是在第y层摔碎了，所以需要从x+1层到y-1层挨个扔鸡蛋测试。再加上第x层和第y层扔的两次（即可以理解为从第x层到第y层均做了测试），总共就是 $y-x+1$ 次。</li>
</ul>
<p>令两种情况相等，即 $x = y-x+1$ ，亦即 $y = 2x - 1$ 。因此第一次扔和第二次扔相差 $y - x$ ，也就是 $x - 1$ 层。</p>
<p>依此类推，假设第三次扔的楼层为z，讨论第二次扔鸡蛋的情况：</p>
<blockquote>
<p>这里<strong>务必要注意与第一次扔的区别</strong>：第一次扔时起始楼层是1，第二次扔时起始楼层是x+1而非x，因为x已经测试过了，这就会导致之后在扔时都会比前一次少个1。举个例子：</p>
<ul>
<li>比如第一次在第x=10层扔，鸡蛋碎了，那么需要从第1层开始测试，直到第9层，因此总共需要1+9=10次；</li>
<li>根据上面讨论，第二次将在 2x-1 = 19层测试，如果鸡蛋碎了，说明N在11~19层之间。因此需要从第11层开始扔，直到第18层，因此总共需要测试1+8=9次。</li>
</ul>
<p>这就是第一次扔与之后再扔的区别。</p>
<ul>
<li>第一次在x层扔时如果鸡蛋碎了，是从第1层开始测试，直到x-1层，那么带上第x层扔的就是x次。</li>
<li>而第二次在y层扔时如果鸡蛋碎了，只需从上一次扔的楼层开始测试，而x已经测试过了，因此是从x+1层到y-1层，带上第y层扔的就是 y - (x+1) + 1 = y - x 次。</li>
</ul>
</blockquote>
<ul>
<li>如果鸡蛋在第y层碎了，那么需要逐个测试x+1~y-1层，再带上第y层扔的一次，总共需测试 1 + (y-1) - (x+1) + 1 = y-x = x-1 次；</li>
<li>如果鸡蛋在第y层没碎，那么需要在第z层继续测试，也就是 $z - y + 1$ 次</li>
</ul>
<p>因此有 $y - x = z - y + 1$ ，得 $z = 2y - x - 1 = 3x-3$。与上一此扔相差 $z - y$ ，也就是 $x-2$ 层。 </p>
<p>依次类推，每次扔的楼层之间相差的层数逐次减1，也就是 x, x-1, x-2 , …, 1。</p>
<p>这就确保了两种情况所需的测试次数是相等的，即保持了均衡。同时为了保证最后一次测试要覆盖100层楼，所以有 $x + (x-1) + (x-2) + \dots + 1 = (x+1)x/2 \geq 100$ ，解得 $x \geq 14$ 。</p>
<p>因此，每次测试的楼层为 14,27,39,50,60,69,77,84,90,95,99,100。</p>
<p>这样就能够保证，如果鸡蛋一直没碎，最后需要测试12次，而在前面无论哪一层碎了，总共都只需要测试14次。</p>
<p><strong>【解法2】</strong></p>
<p>解法1写的过于精细，或者说麻烦了。实际上，当第一次在x层扔时鸡蛋没碎，那么第二次再扔时跟前面1 ~ x层已经没有关系了，相当于以后只需关注从第x+1层到第100层即可，相当于整个大楼的1 ~ x层被移走了，不再要了，只是占了一次测试机会而已。</p>
<p>因为我们目标是让两种情况所需测试次数尽可能相等。</p>
<ul>
<li>在第x层扔时鸡蛋碎了时需要测试x次，所以我们只要保证当鸡蛋没碎时总共只用测试x次即可，而这时前面已经用掉一次测试机会了，也就是说，从第x+1层开始，我们只能测试x-1次，所以第二次测试应该在(x+1)~(x+x-1)之间进行，即第二次应在 x + x - 1 层扔鸡蛋。</li>
<li>第三次扔时跟前面的 x + x - 1 层已经没关系了，应该从下一层开始， 且因为又用掉了一次机会，第三次只能测试x-2次了。</li>
<li>依次类推，下一次 x-3, x-4, …, 1。</li>
</ul>
<p>每次扔砍掉底部的楼层，最后就有 $x + (x-1) + (x-2) + \dots + 1 = (x+1)x/2 \geq 100$ ，解得 $x \geq 14$ 。</p>
<blockquote>
<p>两种思路的不同之处在于：</p>
<ul>
<li>解法1是设每次扔的楼层，利用「两种情况测试次数相同」这个我们期望的条件来建立等式关系，进而求得下一层该在哪扔。</li>
<li>解法2则是把这个等式关系作为已知条件，反推出两层扔的楼层之间的间隔差为1。</li>
</ul>
</blockquote>
<p><strong>【拓展】</strong></p>
<p>本问题可以使用动态规划的方法，推广到n层楼，m个鸡蛋。假设 $dp[n, m]$ 表示n层楼、m个鸡蛋时找到最高楼层的最少尝试次数。</p>
<ul>
<li>当第一个鸡蛋从第i层扔下，如果碎了，还剩下m-1个鸡蛋，且N应该在1~i之间，因此还需要 $dp[i-1,m-1]$ 次，找到子问题；</li>
<li>如果鸡蛋没碎，说明N在i+1~n层之间，因此还需要 $dp[n-i, m]$ 次，得到另一个子问题。</li>
</ul>
<p>综上，状态转移方程如下：</p>
<p>$$dp[n, m] = min_{i=1,…n} { 1 + max(dp[i-1,m-1], dp[n-i, m]) }$$</p>
<p>问题解决。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jay</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://jay1zhang.github.io/2021/06/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/">http://jay1zhang.github.io/2021/06/01/Computer%20Science/Machine%20Learning/%E3%80%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8D%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="http://jayyy1.gitee.io/images/pictures/cartoons/030.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/06/12/Review/%E3%80%8C%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94%E3%80%8D%E6%96%B0%E6%89%8B%E7%9B%B8%E6%9C%BA%E9%80%89%E8%B4%AD%E6%8C%87%E5%8D%97/"><img class="prev-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/026.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">「生活随笔」新手相机选购指南</div></div></a></div><div class="next-post pull-right"><a href="/2021/05/11/Computer%20Science/Deep%20Learning/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%20Yolo/"><img class="next-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/035.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">「深度学习」（目标检测算法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/03/27/Computer Science/Machine Learning/「机器学习」最优化问题之拉格朗日对偶性/" title="「机器学习」（TODO）最优化问题之拉格朗日乘子法"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/038.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-27</div><div class="title">「机器学习」（TODO）最优化问题之拉格朗日乘子法</div></div></a></div><div><a href="/2021/05/01/Computer Science/Machine Learning/「机器学习」过拟合问题与正则化方法/" title="「机器学习」过拟合问题与正则化方法"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/014.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-01</div><div class="title">「机器学习」过拟合问题与正则化方法</div></div></a></div><div><a href="/2022/01/06/Computer Science/Machine Learning/「机器学习」《机器学习》笔记/" title="「机器学习」《机器学习》笔记"><img class="cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/039.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-06</div><div class="title">「机器学习」《机器学习》笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-text">机器学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86%E4%B8%8E%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-text">验证集与测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-%E4%B8%8E-Variance"><span class="toc-text">Bias 与 Variance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="toc-text">极大似然估计与贝叶斯估计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="toc-text">贝叶斯估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">二者的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E8%80%85%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-text">二者的联系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">最小二乘法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E8%A7%A3%E6%AD%A5%E9%AA%A4"><span class="toc-text">求解步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E4%B8%8E%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="toc-text">最小二乘法与最大似然估计的区别与联系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">关于损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">交叉熵损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E9%87%8F"><span class="toc-text">信息量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89"><span class="toc-text">相对熵（KL散度）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-text">交叉熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MAE%E4%B8%8EMSE%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">MAE与MSE的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E9%A2%98"><span class="toc-text">经典算法题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#100%E5%B1%82%E6%A5%BC%E6%89%94%E9%B8%A1%E8%9B%8B%EF%BC%8C%E7%A1%AE%E5%AE%9A%E9%B8%A1%E8%9B%8B%E6%91%94%E7%A2%8E%E7%9A%84%E6%A5%BC%E5%B1%82N"><span class="toc-text">100层楼扔鸡蛋，确定鸡蛋摔碎的楼层N</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/012.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Jay</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>