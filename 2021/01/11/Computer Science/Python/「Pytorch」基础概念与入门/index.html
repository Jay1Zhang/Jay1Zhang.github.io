<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>「Pytorch」基础概念与入门 | J1z's Blog</title><meta name="keywords" content="Pytorch"><meta name="author" content="Jay"><meta name="copyright" content="Jay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="本文整理自 PyTorch 深度学习：60分钟快速入门  原文：https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;deep_learning_60min_blitz.html 作者：Soumith Chintala   视频：https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;u7x8RXwLKcA  0x00 Why Pytorch？PyTorch 是基于以下">
<meta property="og:type" content="article">
<meta property="og:title" content="「Pytorch」基础概念与入门">
<meta property="og:url" content="http://jay1zhang.github.io/2021/01/11/Computer%20Science/Python/%E3%80%8CPytorch%E3%80%8D%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="J1z&#39;s Blog">
<meta property="og:description" content="本文整理自 PyTorch 深度学习：60分钟快速入门  原文：https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;deep_learning_60min_blitz.html 作者：Soumith Chintala   视频：https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;u7x8RXwLKcA  0x00 Why Pytorch？PyTorch 是基于以下">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/020.jpg">
<meta property="article:published_time" content="2021-01-11T08:52:32.000Z">
<meta property="article:modified_time" content="2021-04-06T10:30:35.100Z">
<meta property="article:author" content="Jay">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jayyy1.gitee.io/images/pictures/cartoons/020.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://jay1zhang.github.io/2021/01/11/Computer%20Science/Python/%E3%80%8CPytorch%E3%80%8D%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%A5%E9%97%A8/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"WKEK6XV2V5","apiKey":"e093fbb9f29fdf5c7ddd56ec43e9ae05","indexName":"Jay","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-04-06 18:30:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">51</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/020.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">J1z's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-heart"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">「Pytorch」基础概念与入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-01-11T08:52:32.000Z" title="Created 2021-01-11 16:52:32">2021-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-04-06T10:30:35.100Z" title="Updated 2021-04-06 18:30:35">2021-04-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/">Computer Science</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/Python/">Python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">4.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>18min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本文整理自 <a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/docs/1.7/02.html">PyTorch 深度学习：60分钟快速入门</a></p>
<blockquote>
<p>原文：<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a></p>
<p>作者：<a target="_blank" rel="noopener" href="http://soumith.ch/">Soumith Chintala</a>  </p>
<p>视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/embed/u7x8RXwLKcA">https://www.youtube.com/embed/u7x8RXwLKcA</a></p>
</blockquote>
<h2 id="0x00-Why-Pytorch？"><a href="#0x00-Why-Pytorch？" class="headerlink" title="0x00 Why Pytorch？"></a>0x00 Why Pytorch？</h2><p>PyTorch 是基于以下两个目的而打造的python科学计算框架：</p>
<ul>
<li>无缝替换NumPy，并且通过利用GPU的算力来实现神经网络的加速。</li>
<li>通过自动微分机制，来让神经网络的实现变得更加容易。</li>
</ul>
<h2 id="0x01-What-is-Tensor？"><a href="#0x01-What-is-Tensor？" class="headerlink" title="0x01 What is Tensor？"></a>0x01 What is Tensor？</h2><blockquote>
<p>原文: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py">https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py</a></p>
</blockquote>
<p>张量如同数组和矩阵一样，是一种特殊的数据结构。在<code>PyTorch</code>中, 神经网络的输入、输出以及网络的参数等数据，都是使用张量来进行描述。</p>
<p>张量的使用和 <code>Numpy</code> 中的 <code>ndarrays</code> 很类似，区别在于张量可以在 <code>GPU</code> 或其它专用硬件上运行, 这样可以得到更快的加速效果。如果你对<code>ndarrays</code>很熟悉的话， 张量的使用对你来说就很容易了。如果不太熟悉的话，希望这篇有关张量<code>API</code>的快速入门教程能够帮到你。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<h3 id="1-Tensor的初始化"><a href="#1-Tensor的初始化" class="headerlink" title="1. Tensor的初始化"></a>1. Tensor的初始化</h3><p>张量有很多种不同的初始化方法, 先来看看四个简单的例子：</p>
<p><strong>1. 直接生成张量</strong></p>
<p>由原始数据直接生成张量, 张量类型由原始数据类型决定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure>
<p><strong>2. 通过Numpy数组来生成张量</strong></p>
<p>由已有的<code>Numpy</code>数组来生成张量(反过来也可以由张量来生成<code>Numpy</code>数组, 参考<a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/docs/1.7/03.html#jump">张量与Numpy之间的转换</a>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure>
<p><strong>3. 通过已有的张量来生成新的张量</strong></p>
<p>新的张量将继承已有张量的数据属性(结构、类型), 也可以重新指定新的数据类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data)   <span class="comment"># 保留 x_data 的属性</span></span><br><span class="line">print(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>)   <span class="comment"># 重写 x_data 的数据类型</span></span><br><span class="line">                                                        int -&gt; float</span><br><span class="line">print(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[<span class="number">0.0381</span>, <span class="number">0.5780</span>],</span><br><span class="line">         [<span class="number">0.3963</span>, <span class="number">0.0840</span>]])</span><br></pre></td></tr></table></figure>
<p><strong>4. 通过指定数据维度来生成张量</strong></p>
<p><code>shape</code>是元组类型, 用来描述张量的维数, 下面3个函数通过传入<code>shape</code>来指定生成张量的维数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[<span class="number">0.0266</span>, <span class="number">0.0553</span>, <span class="number">0.9843</span>],</span><br><span class="line">         [<span class="number">0.0398</span>, <span class="number">0.8964</span>, <span class="number">0.3457</span>]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>


<h3 id="2-Tensor的属性"><a href="#2-Tensor的属性" class="headerlink" title="2. Tensor的属性"></a>2. Tensor的属性</h3><p>从张量属性我们可以得到张量的维数、数据类型以及它们所存储的设备(CPU或GPU)。</p>
<p>来看一个简单的例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([<span class="number">3</span>, <span class="number">4</span>])   <span class="comment"># 维数</span></span><br><span class="line">Datatype of tensor: torch.float32     <span class="comment"># 数据类型</span></span><br><span class="line">Device tensor <span class="keyword">is</span> stored on: cpu       <span class="comment"># 存储设备</span></span><br></pre></td></tr></table></figure>


<h3 id="3-Tensor运算"><a href="#3-Tensor运算" class="headerlink" title="3. Tensor运算"></a>3. Tensor运算</h3><p>有超过100种张量相关的运算操作, 例如转置、索引、切片、数学运算、线性代数、随机采样等。更多的运算可以在这里<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">查看</a>。</p>
<p>所有这些运算都可以在GPU上运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断当前环境GPU是否可用, 然后将tensor导入GPU内运行</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>


<p>光说不练假把式，接下来的例子一定要动手跑一跑。如果你对Numpy的运算非常熟悉的话, 那tensor的运算对你来说就是小菜一碟。</p>
<p><strong>1. 张量的索引和切片</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span>            <span class="comment"># 将第1列(从0开始)的数据全部赋值为0</span></span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>


<p><strong>2. 张量的拼接</strong></p>
<p>你可以通过<code>torch.cat</code>方法将一组张量按照指定的维度进行拼接, 也可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.stack.html"><code>torch.stack</code></a>方法。这个方法也可以实现拼接操作, 但和<code>torch.cat</code>稍微有点不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line">print(t1)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>


<p><strong>3. 张量的乘积和矩阵乘法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逐个元素相乘结果</span></span><br><span class="line">print(<span class="string">f&quot;tensor.mul(tensor): \n <span class="subst">&#123;tensor.mul(tensor)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># 等价写法:</span></span><br><span class="line">print(<span class="string">f&quot;tensor * tensor: \n <span class="subst">&#123;tensor * tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor.mul(tensor):</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">tensor * tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])Copy</span><br></pre></td></tr></table></figure>
<p>下面写法表示张量与张量的矩阵乘法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f&quot;tensor.matmul(tensor.T): \n <span class="subst">&#123;tensor.matmul(tensor.T)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># 等价写法:</span></span><br><span class="line">print(<span class="string">f&quot;tensor @ tensor.T: \n <span class="subst">&#123;tensor @ tensor.T&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor.matmul(tensor.T):</span><br><span class="line"> tensor([[<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line">tensor @ tensor.T:</span><br><span class="line"> tensor([[<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])Copy</span><br></pre></td></tr></table></figure>


<p><strong>4. 自动赋值运算</strong></p>
<p>自动赋值运算通常在方法后有 <code>_</code> 作为后缀，例如： <code>x.copy_(y)</code>, <code>x.t_()</code>操作会改变 <code>x</code> 的取值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">6.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意:</p>
<p>自动赋值运算虽然可以节省内存, 但在求导时会因为丢失了中间过程而导致一些问题, 所以我们并不鼓励使用它。</p>
</blockquote>
<h3 id="4-Tensor与Numpy的转化"><a href="#4-Tensor与Numpy的转化" class="headerlink" title="4. Tensor与Numpy的转化"></a>4. Tensor与Numpy的转化</h3><p>张量和<code>Numpy array</code>数组在CPU上可以共用一块内存区域，改变其中一个另一个也会随之改变。</p>
<p><strong>1. 由张量变换为Numpy array数组</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">n: [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br></pre></td></tr></table></figure>
<p>修改张量的值，则<code>Numpy array</code>数组值也会随之改变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>])</span><br><span class="line">n: [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br></pre></td></tr></table></figure>


<p><strong>2. 由Numpy array数组转为张量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure>
<p>修改<code>Numpy array</code>数组的值，则张量值也会随之改变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out=n)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], dtype=torch.float64)</span><br><span class="line">n: [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br></pre></td></tr></table></figure>


<h2 id="0x02-Brief-introduction-to-torch-autograd"><a href="#0x02-Brief-introduction-to-torch-autograd" class="headerlink" title="0x02 Brief introduction to torch.autograd"></a>0x02 Brief introduction to <code>torch.autograd</code></h2><blockquote>
<p>原文：<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py</a></p>
</blockquote>
<p><code>torch.autograd</code>是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。在本节中，您将获得有关 Autograd 如何帮助神经网络训练的概念性理解。</p>
<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>神经网络（NN）是在某些输入数据上执行的嵌套函数的集合。 这些函数由<strong>参数</strong>（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。</p>
<p>训练 NN 通常分为两个步骤：</p>
<ul>
<li><p><strong>正向传播</strong>：在正向传播中，NN 对正确的输出进行最佳猜测。 它通过其每个函数运行输入数据以进行猜测。</p>
</li>
<li><p><strong>反向传播</strong>：在反向传播中，NN 根据其猜测中的误差调整其参数。 它通过从输出向后遍历，收集有关函数参数的误差导数（即<em>梯度</em>）并使用梯度下降来优化参数来实现。 </p>
<blockquote>
<p>有关反向传播的更详细的演练，请查看 3Blue1Brown 的<a target="_blank" rel="noopener" href="https://space.bilibili.com/88461692?from=search&seid=15082561093676673372">B站视频</a> / <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tIeHLnjs5U8">Youtube视频</a>。（Youtube比B站更全一些）</p>
</blockquote>
</li>
</ul>
<h3 id="2-在-PyTorch-中的用法"><a href="#2-在-PyTorch-中的用法" class="headerlink" title="2. 在 PyTorch 中的用法"></a>2. 在 PyTorch 中的用法</h3><p>让我们来看一个训练步骤。 对于此示例，我们从<code>torchvision</code>加载了经过预训练的 resnet18 模型。 </p>
<p>我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&amp;宽度为 64，其对应的<code>label</code>初始化为一些随机值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch, torchvision</span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">labels = torch.rand(<span class="number">1</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>我们通过模型的每一层运行输入数据以进行预测，这是<strong>正向传播</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = model(data) <span class="comment"># forward pass</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们使用模型的预测和对应的标签来计算误差（<code>loss</code>），并通过网络<strong>反向传播</strong>此误差。 </p>
<p>当我们在误差张量上调用<code>.backward()</code>时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的<code>.grad</code>属性中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = (prediction - labels).<span class="built_in">sum</span>()</span><br><span class="line">loss.backward() <span class="comment"># backward pass</span></span><br></pre></td></tr></table></figure>
<p>下一步，我们需要加载一个优化器，并在优化器中注册模型的所有参数。本例中优化器为 SGD，学习率为 0.01，动量为 0.9。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>最后，我们调用<code>.step()</code>启动梯度下降。 优化器通过<code>.grad</code>中存储的梯度来调整每个参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim.step() <span class="comment"># gradient descent</span></span><br></pre></td></tr></table></figure>
<p>至此，您已经具备了训练神经网络所需的一切。 </p>
<h3 id="3-计算图"><a href="#3-计算图" class="headerlink" title="3. 计算图"></a>3. 计算图</h3><p>从概念上讲，Autograd 在由<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function">函数</a>对象组成的有向无环图（DAG）中记录数据（张量）和所有已执行的操作（以及由此产生的新张量）。 在此 DAG 中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。</p>
<p>在正向传播中，Autograd 同时执行两项操作：</p>
<ul>
<li>运行请求的操作以计算结果张量；</li>
<li>在 DAG 中维护操作的<em>梯度函数</em>。</li>
</ul>
<p>当在 DAG 的输出结点（一般是<code>loss</code>）上调用 <code>.backward()</code> 时，反向传播开始。Autograd 将执行：</p>
<ul>
<li>从每个<code>.grad_fn</code>计算梯度，并将它们累积在各自的张量的<code>.grad</code>属性中；</li>
<li>使用链式规则，一直传播到叶子张量。</li>
</ul>
<h3 id="4-冻结参数"><a href="#4-冻结参数" class="headerlink" title="4. 冻结参数"></a>4. 冻结参数</h3><p>在 NN 中，不计算梯度的参数通常称为<strong>冻结参数</strong>。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分很有用（通过减少自动梯度计算，这会带来一些表现优势）。</p>
<p><code>torch.autograd</code>跟踪所有将其<code>requires_grad</code>标志设置为<code>True</code>的张量的操作。 对于不需要梯度的张量，将此属性设置为<code>False</code>会将其从梯度计算 DAG 中排除。</p>
<p>即使只有一个输入张量具有<code>requires_grad=True</code>，操作的输出张量也将需要梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">z = torch.rand((<span class="number">5</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = x + y</span><br><span class="line">print(<span class="string">f&quot;Does `a` require gradients? : <span class="subst">&#123;a.requires_grad&#125;</span>&quot;</span>)</span><br><span class="line">b = x + z</span><br><span class="line">print(<span class="string">f&quot;Does `b` require gradients?: <span class="subst">&#123;b.requires_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>显示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Does &#96;a&#96; require gradients? : False</span><br><span class="line">Does &#96;b&#96; require gradients?: True</span><br></pre></td></tr></table></figure>


<p>冻结参数的一个常见用例是<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html">调整预训练网络</a>。在微调中，我们冻结了大部分模型，通常仅修改分类器层以对新标签进行预测。 </p>
<p>让我们来看一个例子来说明这一点。 和以前一样，我们加载一个预训练的 resnet18 模型，并冻结所有参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze all the parameters in the network</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>假设我们要在具有 10 个标签的新数据集中微调模型。 因此我们在 resnet 的最后增加一个一个线性层 <code>model.fc</code> 作为分类器。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>现在，除了<code>model.fc</code>的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是<code>model.fc</code>的权重和偏差。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)Copy</span><br></pre></td></tr></table></figure>
<p>请注意，尽管我们在优化器中注册了所有参数，但唯一可计算梯度的参数（因此会在梯度下降中进行更新）是分类器的权重和偏差。</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html"><code>torch.no_grad()</code></a>中的上下文管理器也可以实现相同的冻结功能。</p>
<h3 id="5-扩展阅读"><a href="#5-扩展阅读" class="headerlink" title="5. 扩展阅读"></a>5. 扩展阅读</h3><p>以下各节详细介绍了 Autograd 的工作原理，可以选择跳过它们。</p>
<ul>
<li><p>Autograd的原理</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/autograd.html">原地操作&amp;多线程 Autograd</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC">反向模式自动微分</a> 的示例实现</p>
</li>
</ul>
<h4 id="Autograd-的原理"><a href="#Autograd-的原理" class="headerlink" title="Autograd 的原理"></a>Autograd 的原理</h4><p>在本节，让我们来看看<code>autograd</code>是如何收集梯度的。 </p>
<p>首先我们用 <code>requires_grad=True</code> 来创建两个张量 $a$ 和 $b$。 这向<code>autograd</code>发出信号，应跟踪对它们的所有操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">6.</span>, <span class="number">4.</span>], requires_grad=<span class="literal">True</span>)Copy</span><br></pre></td></tr></table></figure>
<p>接下来我们从 $a$ 和 $b$ 创建另一个张量 $Q = 3a^3 - b^2$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q = <span class="number">3</span>*a**<span class="number">3</span> - b**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>我们假设 <code>a</code> 和 <code>b</code> 是神经网络的参数，<code>Q</code>是误差。 在 NN 训练中，我们想要相对于参数的误差，即偏导数： </p>
<p>$$\frac{\partial Q}{\partial a} = 9a^2$$</p>
<p>$$\frac{\partial Q}{\partial b} = -2b$$</p>
<p>当我们在<code>Q</code>上调用<code>.backward()</code>时，Autograd 将计算这些梯度并将其存储在各个张量的<code>.grad</code>属性中。</p>
<p>我们需要在<code>Q.backward()</code>中显式传递<code>gradient</code>参数，因为它是向量。 <code>gradient</code>是与<code>Q</code>形状相同的张量，它表示<code>Q</code>相对于本身的梯度，即 $$\frac{\partial Q}{\partial Q} = 1$$</p>
<p>同样，我们也可以将<code>Q</code>聚合为一个标量，然后隐式地向后调用，例如<code>Q.sum().backward()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">external_grad = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">Q.backward(gradient=external_grad)</span><br></pre></td></tr></table></figure>
<p>现在，梯度已经保存在<code>a.grad</code>和<code>b.grad</code>中了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check if collected gradients are correct</span></span><br><span class="line">print(<span class="number">9</span>*a**<span class="number">2</span> == a.grad)</span><br><span class="line">print(-<span class="number">2</span>*b == b.grad)</span><br></pre></td></tr></table></figure>
<p>显示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>


<h2 id="0x03-How-to-define-a-Neural-Network"><a href="#0x03-How-to-define-a-Neural-Network" class="headerlink" title="0x03 How to define a Neural Network"></a>0x03 How to define a Neural Network</h2><blockquote>
<p>原文：<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py</a></p>
</blockquote>
<p>现在您已经了解了<code>autograd</code>，可以使用<code>torch.nn</code>包构建神经网络。<code>nn</code>依赖于<code>autograd</code>来定义模型并对其进行微分。 <code>nn.Module</code>包含了各种神经网络层，以及从 <code>input</code> 生成 <code>output</code> 的方法： <code>forward()</code> 。</p>
<p>神经网络的典型训练过程如下：</p>
<ol>
<li>定义具有一些可学习参数（或权重）的神经网络</li>
<li>遍历输入数据集，通过网络处理输入</li>
<li>计算损失（输出正确的距离有多远）</li>
<li>将梯度反向传播回网络参数</li>
<li>通常使用简单的更新规则来更新网络的权重：<code>weight = weight - learning_rate * gradient</code></li>
</ol>
<h3 id="1-定义神经网络"><a href="#1-定义神经网络" class="headerlink" title="1. 定义神经网络"></a>1. 定义神经网络</h3><p>下面我们定义一个简单地CNN网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 3x3 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">120</span>)  <span class="comment"># 6*6 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">576</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)Copy</span><br></pre></td></tr></table></figure>


<p>我们只需要定义好 <code>forward</code> 函数，就可以使用 <code>autograd</code> 自动定义<code>backward</code>函数来计算梯度。 在<code>forward</code>函数中可以进行任何张量操作。</p>
<p>模型的可学习参数由<code>net.parameters()</code>返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line">print(<span class="built_in">len</span>(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line">torch.Size([<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>


<h3 id="2-处理输入"><a href="#2-处理输入" class="headerlink" title="2. 处理输入"></a>2. 处理输入</h3><p>让我们尝试一个<code>32x32</code>随机输入。 注意：该网络的预期输入大小（LeNet）为<code>32x32</code>。 要在 MNIST 数据集上使用此网络，请将图像从数据集中调整为<code>32x32</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.1002</span>, -<span class="number">0.0694</span>, -<span class="number">0.0436</span>,  <span class="number">0.0103</span>,  <span class="number">0.0488</span>, -<span class="number">0.0429</span>, -<span class="number">0.0941</span>, -<span class="number">0.0146</span>,</span><br><span class="line">         -<span class="number">0.0031</span>, -<span class="number">0.0923</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>使用随机梯度将所有参数和反向传播的梯度缓冲区归零：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ul>
<li><p><code>torch.nn</code>仅支持小批量而不是单个样本的输入。</p>
</li>
<li><p>例如，<code>nn.Conv2d</code>将采用<code>nSamples x nChannels x Height x Width</code>的 4D 张量。如果您只有一个样本，只需使用<code>input.unsqueeze(0)</code>添加一个<strong>假批量尺寸</strong>。</p>
</li>
</ul>
<h3 id="3-损失函数与计算图"><a href="#3-损失函数与计算图" class="headerlink" title="3. 损失函数与计算图"></a>3. 损失函数与计算图</h3><p>损失函数采用一对（输出，目标）输入，并计算一个值，该值估计输出与目标之间的距离。</p>
<p><code>nn</code>包下有几种不同的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/nn.html#loss-functions">损失函数</a>。 一个简单的损失是：<code>nn.MSELoss</code>，它计算输入和目标之间的均方误差。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.4969</span>, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure>


<p>现在，如果使用<code>.grad_fn</code>属性向后跟随<code>loss</code>，您将看到一个计算图，如下所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
<p>因此，当我们调用<code>loss.backward()</code>时，整个图将被微分，并且图中具有 <code>requires_grad=True</code> 的所有张量将随梯度累积其<code>.grad</code>张量。</p>
<p>为了说明，让我们向后走几步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;MseLossBackward <span class="built_in">object</span> at <span class="number">0x7f1ba05a1ba8</span>&gt;</span><br><span class="line">&lt;AddmmBackward <span class="built_in">object</span> at <span class="number">0x7f1ba05a19e8</span>&gt;</span><br><span class="line">&lt;AccumulateGrad <span class="built_in">object</span> at <span class="number">0x7f1ba05a19e8</span>&gt;</span><br></pre></td></tr></table></figure>


<h3 id="4-反向传播误差"><a href="#4-反向传播误差" class="headerlink" title="4. 反向传播误差"></a>4. 反向传播误差</h3><p>要反向传播误差，我们要做的只是对<code>loss.backward()</code>。 不过，您需要清除现有的梯度，否则梯度将累积到现有的梯度中。</p>
<p>现在，我们将其称为<code>loss.backward()</code>，然后看一下向后前后<code>conv1</code>的偏差梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([ <span class="number">0.0111</span>, -<span class="number">0.0064</span>,  <span class="number">0.0053</span>, -<span class="number">0.0047</span>,  <span class="number">0.0026</span>, -<span class="number">0.0153</span>])</span><br></pre></td></tr></table></figure>


<h3 id="5-更新权重"><a href="#5-更新权重" class="headerlink" title="5. 更新权重"></a>5. 更新权重</h3><p>实践中使用的最简单的更新规则是随机梯度下降（SGD）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight &#x3D; weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>
<p>我们可以使用简单的 Python 代码实现此目标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)Copy</span><br></pre></td></tr></table></figure>
<p>但是，在使用神经网络时，您可能希望使用各种不同的更新规则，例如 SGD，Nesterov-SGD，Adam，RMSProp 等。</p>
<p>为实现此目的，我们构建了一个小包装：<code>torch.optim</code>，可实现所有这些方法。 使用它非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<p>注意观察如何使用<code>optimizer.zero_grad()</code>将梯度缓冲区手动设置为零。 这是因为如<a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/docs/1.7/05.html#backprop">反向传播</a>部分中所述累积了梯度。</p>
<h2 id="0x04-Practice-training-a-image-classifier"><a href="#0x04-Practice-training-a-image-classifier" class="headerlink" title="0x04 Practice: training a image classifier"></a>0x04 Practice: training a image classifier</h2></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jay</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://jay1zhang.github.io/2021/01/11/Computer%20Science/Python/%E3%80%8CPytorch%E3%80%8D%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%A5%E9%97%A8/">http://jay1zhang.github.io/2021/01/11/Computer%20Science/Python/%E3%80%8CPytorch%E3%80%8D%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%A5%E9%97%A8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="http://jayyy1.gitee.io/images/pictures/cartoons/020.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/01/14/Computer%20Science/Python/%E3%80%8CTelBot%E3%80%8D%E6%89%93%E9%80%A0%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8EPTB%E7%9A%84Telegram%20Bot/"><img class="prev-cover" src="http://jayyy1.gitee.io/images/pictures/cartoons/035.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">「TelBot」打造一个基于PTB的Telegram Bot</div></div></a></div><div class="next-post pull-right"><a href="/2021/01/10/Review/%E3%80%8C%E6%82%A6%E8%AF%BB%E6%84%9F%E6%82%9F%E3%80%8DKnowing%20She%20Would/"><img class="next-cover" src="https://gitee.com/Jayyy1/images/raw/master/posts/Review/image-20210110124531629.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">「悦读感悟」Knowing She Would —— 《挪威的森林》摘录</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0x00-Why-Pytorch%EF%BC%9F"><span class="toc-text">0x00 Why Pytorch？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x01-What-is-Tensor%EF%BC%9F"><span class="toc-text">0x01 What is Tensor？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Tensor%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">1. Tensor的初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Tensor%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-text">2. Tensor的属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Tensor%E8%BF%90%E7%AE%97"><span class="toc-text">3. Tensor运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Tensor%E4%B8%8ENumpy%E7%9A%84%E8%BD%AC%E5%8C%96"><span class="toc-text">4. Tensor与Numpy的转化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x02-Brief-introduction-to-torch-autograd"><span class="toc-text">0x02 Brief introduction to torch.autograd</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%83%8C%E6%99%AF"><span class="toc-text">1. 背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%9C%A8-PyTorch-%E4%B8%AD%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-text">2. 在 PyTorch 中的用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">3. 计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0"><span class="toc-text">4. 冻结参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB"><span class="toc-text">5. 扩展阅读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Autograd-%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">Autograd 的原理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x03-How-to-define-a-Neural-Network"><span class="toc-text">0x03 How to define a Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">1. 定义神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A4%84%E7%90%86%E8%BE%93%E5%85%A5"><span class="toc-text">2. 处理输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">3. 损失函数与计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%AF%E5%B7%AE"><span class="toc-text">4. 反向传播误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%9B%B4%E6%96%B0%E6%9D%83%E9%87%8D"><span class="toc-text">5. 更新权重</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x04-Practice-training-a-image-classifier"><span class="toc-text">0x04 Practice: training a image classifier</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url(http://jayyy1.gitee.io/images/pictures/cartoons/012.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jay</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>